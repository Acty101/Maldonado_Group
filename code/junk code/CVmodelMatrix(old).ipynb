{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkFv6iYwhmFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74da6468-9285-428b-9802-0732e15d8df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Maldonado Lab\n"
          ]
        }
      ],
      "source": [
        "from prompt_toolkit.application import current\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from google.colab import drive\n",
        "\n",
        "# REQUIRES: Folder path must be 1 directories above .xlsx files ({INPUT_PATH} -- > xlsx files)\n",
        "# EFFECT: Reads all xlsx files and returns a 3D numpy array (batch_size, x,y) with data stacked along new axis (axis = 0)\n",
        "def excel_reader(folder_path): \n",
        "    files = glob.glob(folder_path + \"/*.xlsx\")\n",
        "    df_list = []\n",
        "    for file in files:\n",
        "        df = pd.read_excel(file)\n",
        "        df_list.append(df)\n",
        "    np_array = np.array(list(map(lambda x: x.to_numpy(), df_list)))\n",
        "    np_array = np_array.reshape((len(df_list), len(df.axes[0]), len(df.axes[1])))\n",
        "    return np_array\n",
        "\n",
        "# EFFECT: Returns list of non-hidden folders & files present in a directory \n",
        "def listdir_non_hidden(folder_path): \n",
        "    folders = []\n",
        "    for folder in os.listdir(folder_path):\n",
        "        if not folder.startswith('.'):\n",
        "            folders.append(folder)\n",
        "    return folders\n",
        "\n",
        "# REQUIRES: x,y,z be not empty\n",
        "# EEFECT: generate model structure based on input shape of x,y,z\n",
        "def create_model(datas, x,y,z):\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Conv2D(32, (3, 3), activation='relu', input_shape=(x,y,z)),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(100, activation='relu'),\n",
        "      layers.Dense(len(datas)*4, activation = 'relu'),\n",
        "      layers.Dense(len(datas)),\n",
        "      layers.Softmax()]) # UNCERTAINTY: is softmax allowed here?\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "      \n",
        "  return model\n",
        "\n",
        "# REQUIRES: Model weights in folder must match current model architecture\n",
        "# EFFECT: Load a model from its saved weights\n",
        "def load_model(path, x,y,z):\n",
        "  model = create_model(x,y,z)\n",
        "  model.load_weights(path)\n",
        "  return model\n",
        "\n",
        "# REQUIRES: - Maximum and minimum values are more than and less than 0 respectively \n",
        "#           - Folder path must be 2 directories above .xlsx files ({INPUT_PATH} -- > categories -- > xlsx files)\n",
        "# EFFECT: Summarizes data of xslx files in a folder by returning four items: \n",
        "#         1. maximum value, 2. minimum value, 3. regular array whose elements are numpy datas corresponding to categories (e.g. [np1,np2,np3] ), 4. folder categories\n",
        "def summarize(path):\n",
        "  folders = listdir_non_hidden(path)\n",
        "  max_val = 0\n",
        "  min_val = 0\n",
        "  datas = []\n",
        "  for folder in folders:\n",
        "    data = excel_reader(path + folder)\n",
        "    current_max = data.max()\n",
        "    current_min = data.min()\n",
        "    if(max_val < current_max):\n",
        "      max_val = current_max\n",
        "    if(min_val > current_min):\n",
        "      min_val = current_min\n",
        "    datas.append(data)\n",
        "  return datas, max_val, min_val, folders\n",
        "\n",
        "# REQUIRES: Index >= 0, predictions >= 3, and choices reflect number of output nodes for model\n",
        "# EFFECT: Prints top 3 predictions for a given data\n",
        "def predict_top_three(index, predictions, choices):\n",
        "  prediction = []\n",
        "  for i in range(len(predictions[index])):\n",
        "    prediction.append(predictions[index][i])\n",
        "  prediction = np.asarray(prediction)\n",
        "  ranks = []\n",
        "  for i in range(3):\n",
        "    current_index = np.argmax(prediction)\n",
        "    ranks.append(current_index)\n",
        "    prediction[current_index] = 0\n",
        "  print(\"The top three predictions are:\")\n",
        "  print(\"1.\", choices[ranks[0]], \"with probability of\", predictions[index][ranks[0]])\n",
        "  print(\"2.\", choices[ranks[1]], \"with probability of\", predictions[index][ranks[1]])\n",
        "  print(\"3.\", choices[ranks[2]], \"with probability of\", predictions[index][ranks[2]])\n",
        "  print()\n",
        "  return\n",
        "\n",
        "# REQUIRES: start_index, num_terms > 0, model is trained, and choices reflect number of output nodes for model\n",
        "# EFFECT: Prints top prediction for a given range (start-end)\n",
        "def predict_top_three_multiple(start_index,num_terms,predictions,choices):\n",
        "  range_predictions = predictions[start_index:start_index + num_terms]\n",
        "  for i in range(num_terms):\n",
        "    print(\"Data(\", i, \")\")\n",
        "    predict_top_three(i,range_predictions,choices)\n",
        "  return\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "%cd \"/content/drive/My Drive/Maldonado Lab/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# relative path to training data \n",
        "trn_data_path =  \"matrix-data/\"\n",
        "# relative path to experimental data\n",
        "exp_data_path =  \"matrix-data/\"\n",
        "\n",
        "# gather training datas & make labels\n",
        "trn_datas, max_trn_val, min_trn_val, trn_categories = summarize(trn_data_path)\n",
        "train_data = np.vstack(trn_datas)\n",
        "train_labels = []\n",
        "for i in range(len(trn_datas)):\n",
        "    for j in range(len(trn_datas[i])):\n",
        "        train_labels.append(i)\n",
        "train_labels = np.asarray(train_labels)\n",
        "\n",
        "\n",
        "# gather experimental data \n",
        "exp_datas, max_exp_val, min_exp_val, exp_categories = summarize(exp_data_path)\n",
        "experimental_data = np.vstack(exp_datas)\n",
        "\n",
        "# normalize data (shift range from negative-positive values to 0-1 values)\n",
        "max_val = max(max_trn_val,max_exp_val)\n",
        "min_val = min(min_trn_val,min_exp_val)\n",
        "train_data = (train_data - min_val) / (max_val - min_val)\n",
        "experimental_data = (experimental_data - min_val) / (max_val - min_val)\n",
        "# write max and min value to txt file for future data checking/normalization\n",
        "os.remove('max-min.txt')\n",
        "np.savetxt('max-min.txt',[max_val,min_val])\n",
        "\n",
        "# add dimensionality along last axis (3D --> 4D) since CNN requires 4D input\n",
        "train_data = np.expand_dims(train_data, axis = len(train_data.shape))\n",
        "experimental_data = np.expand_dims(experimental_data, axis = len(experimental_data.shape))\n",
        "\n",
        "print(\"Dimensions of training data:\",train_data.shape)\n",
        "print(\"Dimensions of experimental data:\",experimental_data.shape)\n"
      ],
      "metadata": {
        "id": "RyIB5y1nRkPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make instance of model\n",
        "x,y,z = train_data.shape[1], train_data.shape[2], train_data.shape[3]\n",
        "model = create_model(trn_datas,x,y,z)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JFELY9-qRnxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a84ca5c-8700-4621-e2dc-1712078c3a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 424, 26, 32)       320       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 212, 13, 32)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 210, 11, 64)       18496     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 105, 5, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 103, 3, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 51, 1, 64)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 3264)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100)               326500    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 12)                1212      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 39        \n",
            "                                                                 \n",
            " softmax_1 (Softmax)         (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 383,495\n",
            "Trainable params: 383,495\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train model with 30-70 validation split\n",
        "model.fit(train_data, train_labels, epochs = 50, validation_split = 0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y56WoMq-Tb3p",
        "outputId": "5fdd2798-65ee-483e-fa95-82c80bf3dcf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 1s 998ms/step - loss: 1.0949 - accuracy: 0.5000 - val_loss: 1.2345 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 1.0381 - accuracy: 0.5000 - val_loss: 1.3546 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 0.9970 - accuracy: 0.5000 - val_loss: 1.4385 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.9773 - accuracy: 0.5000 - val_loss: 1.4506 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.9613 - accuracy: 0.5000 - val_loss: 1.4592 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.9619 - accuracy: 0.5000 - val_loss: 1.4600 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.9608 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.9584 - accuracy: 0.5000 - val_loss: 1.4629 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.9630 - accuracy: 0.5000 - val_loss: 1.4592 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.9593 - accuracy: 0.5000 - val_loss: 1.4597 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.9597 - accuracy: 0.5000 - val_loss: 1.4623 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 0.9623 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.9581 - accuracy: 0.5000 - val_loss: 1.4614 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.9614 - accuracy: 0.5000 - val_loss: 1.4597 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.9597 - accuracy: 0.5000 - val_loss: 1.4587 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.9587 - accuracy: 0.5000 - val_loss: 1.4611 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.9611 - accuracy: 0.5000 - val_loss: 1.4580 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.9580 - accuracy: 0.5000 - val_loss: 1.4603 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.9603 - accuracy: 0.5000 - val_loss: 1.4588 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.9588 - accuracy: 0.5000 - val_loss: 1.4589 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.9589 - accuracy: 0.5000 - val_loss: 1.4595 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 0.9595 - accuracy: 0.5000 - val_loss: 1.4580 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.9580 - accuracy: 0.5000 - val_loss: 1.4592 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 0.9592 - accuracy: 0.5000 - val_loss: 1.4586 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.9586 - accuracy: 0.5000 - val_loss: 1.4583 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 0.9583 - accuracy: 0.5000 - val_loss: 1.4590 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 0.9589 - accuracy: 0.5000 - val_loss: 1.4580 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.9580 - accuracy: 0.5000 - val_loss: 1.4587 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 195ms/step - loss: 0.9587 - accuracy: 0.5000 - val_loss: 1.4582 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.9582 - accuracy: 0.5000 - val_loss: 1.4583 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.9583 - accuracy: 0.5000 - val_loss: 1.4584 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.9584 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.9581 - accuracy: 0.5000 - val_loss: 1.4585 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.9585 - accuracy: 0.5000 - val_loss: 1.4580 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 206ms/step - loss: 0.9580 - accuracy: 0.5000 - val_loss: 1.4584 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.9584 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.9581 - accuracy: 0.5000 - val_loss: 1.4582 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 211ms/step - loss: 0.9582 - accuracy: 0.5000 - val_loss: 1.4582 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.9582 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.9581 - accuracy: 0.5000 - val_loss: 1.4582 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 0.9582 - accuracy: 0.5000 - val_loss: 1.4580 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.9580 - accuracy: 0.5000 - val_loss: 1.4582 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 210ms/step - loss: 0.9582 - accuracy: 0.5000 - val_loss: 1.4580 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 207ms/step - loss: 0.9580 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.9581 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 201ms/step - loss: 0.9581 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.9581 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.9581 - accuracy: 0.5000 - val_loss: 1.4580 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.9580 - accuracy: 0.5000 - val_loss: 1.4581 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 0.9581 - accuracy: 0.5000 - val_loss: 1.4580 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f03890ab8e0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if validation is good (>90%) --> save weights in path below\n",
        "save_to_path = \n",
        "model.save_weights(save_to_path)"
      ],
      "metadata": {
        "id": "Wmpa0eA6U2Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS IF: model was previously saved and would only like to predict results on new experimental data\n",
        "trn_data_path = \n",
        "exp_data_path =  \n",
        "saved_path_weights = \n",
        "\n",
        "# load experimental data & model\n",
        "exp_datas, max_exp_val, min_exp_val, exp_categories = summarize(exp_data_path)\n",
        "# PITFALL: - ensure max_exp_val and min_exp_val of new batch is less than saved max and min values\n",
        "#          - else, predictions could be inaccurate\n",
        "experimental_data = np.vstack(exp_datas)\n",
        "experimental_data = np.expand_dims(experimental_data, axis = len(experimental_data.shape))\n",
        "choices = listdir_non_hidden(trn_data_path)\n",
        "x,y,z = experimental_data.shape[1], experimental_data.shape[2], experimental_data.shape[3]\n",
        "model = load_model(saved_path_weights, x,y,z)"
      ],
      "metadata": {
        "id": "99Zb4TCIV92R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "predictions = model.predict(experimental_data)\n",
        "predict_top_three(i,predictions,trn_categories)\n",
        "predict_top_three_multiple(i,5,predictions,trn_categories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLGsgDJYTdiy",
        "outputId": "6eea9563-e009-4a89-dd8e-e7c2104aed7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 87ms/step\n",
            "The top three predictions are:\n",
            "1. cat2 with probability of 0.5025637\n",
            "2. cat1 with probability of 0.49743608\n",
            "3. cat3 with probability of 1.8636541e-07\n",
            "\n",
            "Data( 0 )\n",
            "The top three predictions are:\n",
            "1. cat2 with probability of 0.5025637\n",
            "2. cat1 with probability of 0.49743608\n",
            "3. cat3 with probability of 1.8636541e-07\n",
            "\n",
            "Data( 1 )\n",
            "The top three predictions are:\n",
            "1. cat2 with probability of 0.5025624\n",
            "2. cat1 with probability of 0.4974374\n",
            "3. cat3 with probability of 1.86361e-07\n",
            "\n",
            "Data( 2 )\n",
            "The top three predictions are:\n",
            "1. cat2 with probability of 0.50256276\n",
            "2. cat1 with probability of 0.49743706\n",
            "3. cat3 with probability of 1.8635954e-07\n",
            "\n",
            "Data( 3 )\n",
            "The top three predictions are:\n",
            "1. cat2 with probability of 0.50256103\n",
            "2. cat1 with probability of 0.49743888\n",
            "3. cat3 with probability of 1.8635997e-07\n",
            "\n",
            "Data( 4 )\n",
            "The top three predictions are:\n",
            "1. cat2 with probability of 0.50256383\n",
            "2. cat1 with probability of 0.49743596\n",
            "3. cat3 with probability of 1.8636437e-07\n",
            "\n"
          ]
        }
      ]
    }
  ]
}